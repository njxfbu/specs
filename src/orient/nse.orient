Utilities:
  b_in_eib = (1024 * 1024 * 1024 * 1024 * 1024 * 1024)
  b_in_pib = (1024 * 1024 * 1024 * 1024 * 1024)
  b_in_tib = (1024 * 1024 * 1024 * 1024)
  b_in_gib = (1024 * 1024 * 1024)
  b_in_mib = (1024 * 1024)
  gib_in_tib = 1024
  gib_in_pib = 1024 * 1024
  b_in_kib = 1024
  secs_in_month = (24 * 60 * 60 * 30)
  secs_in_year = (24  * 60 * 60 * 365)
  secs_in_day = (24 * 60 * 60)
  secs_in_hour = (60 * 60)
  secs_in_min = (60)
  tb_to_tib = (1024 * 1024 * 1024 * 1024) / (1000 * 1000 * 1000 * 1000)
  gb_to_gib = (1024 * 1024 * 1024) / (1000 * 1000 * 1000)
  mb_to_mib = (1024 * 1024) / (1000 * 1000)


Rigs:
  rig_memaccess_throughput_tib_s = rig_memaccess_throughput_tb_s * tb_to_tib
  rig_memaccess_throughput_b_s = rig_memaccess_throughput_tib_s * b_in_tib
  // rig_memaccess_throughput_access_s = rig_memaccess_throughput_b_s / node_size
  // rig_memaccess_throughput_gaccess_s = rig_memaccess_throughput_access_s / (1000 * 1000 * 1000)  

  rig_hashing_throughput_tib_s = rig_hashing_throughput_tb_s * tb_to_tib
  rig_hashing_throughput_b_s = rig_hashing_throughput_tib_s * b_in_tib
  // rig_hashing_throughput_hash_s = rig_hashing_throughput_b_s / node_size
  // rig_hashing_throughput_ghash_s = rig_hashing_throughput_hash_s / (1000 * 1000 * 1000)  
  describe(rig_cost_s, "Cost of running the rig for one second")
  rig_cost_s = rig_cost / (rig_lifetime_years * secs_in_year)

  rig_cost_storage_tib = rig_cost_storage_tb * tb_to_tib
  rig_cost_storage_tib = rig_cost_storage_b * b_in_tib
  rig_cost_storage_b_s = rig_cost_storage_b / (rig_storage_lifetime_years * secs_in_year)


NSE:
  PaperToUbercalc:
    // T = windows
    // d_e = expander_degree
    // d_b = butterfly_degree
    // l_e = expander_layers
    // l_b = butterfly_layers
    // l = l_e + l_b
    // m = porep_challenges
    // c = post_window_challenges
    // N = window_nodes

  Params:
    describe(window_nodes, "Nodes in a window", nodes)
    window_nodes = window_size / node_size
    node_sequence_size = node_size * nodes_in_sequence
    replica_nodes = window_nodes * windows
    replica_size = window_size * windows
    replica_size_gib = replica_size / b_in_gib
    replica_size_tib = replica_size / b_in_tib

    window_size = window_size_gib * b_in_gib

  PoRep:
    porep_soundness = expt(2, -porep_lambda)
    layers = expander_layers + butterfly_layers
    porep_challenges = -porep_lambda / log2(1- delta / layers)

    Costs:
      cost_replicate = porep_time_replicate * rig_cost_s
      time_memaccess_node =  (node_size) / rig_memaccess_throughput_b_s
      time_memaccess_sequence = time_memaccess_node * nodes_in_sequence
      cost_memaccess_sequence = time_memaccess_sequence * rig_cost_s
      cost_memaccess_node = time_memaccess_node * rig_cost_s

      NSE:
        gamma = 1 - spacegap
        w = windows * (gamma + delta) / 0.9502 //pinsker

        // Attacker's formula TODO: note that expander_layers are - 2 but we count them as -1
        memaccess_sequence_attack = ((windows - w -1)/spacegap) * (expander_layers - 1) * (1 - 0.0249) * window_nodes * expander_degree
        
        cost_attack_replica = memaccess_sequence_attack * cost_memaccess_sequence
        cost_honest_replica = memaccess_replica * cost_memaccess_node
        cost_honest_storage_proving_period = rig_cost_storage_b_s * proving_period_hours * secs_in_hour * replica_size
        cost_honest_storage_proving_period_node = rig_cost_storage_b_s *  proving_period_hours * secs_in_hour * 32
        cost_honest_storage_year = rig_cost_storage_b_s *  secs_in_year * replica_size

        amax = cost_attack_replica / cost_honest_storage_proving_period
        describe(cost_honest_ratio, "Ratio of the cost of replica vs cost of storage for a year")
        cost_honest_ratio = cost_honest_replica / cost_honest_storage_year
    Commitment:
      mtrees = expander_layers + butterfly_layers
      mtree_leaves = replica_nodes * mtrees
      mtree_depth = (log2(mtree_leaves)/log2(mtree_hash_blocks))
      commtree_time = (mtree_hash_time * mtree_leaves) / mtree_hash_blocks)
      porep_time_commitment = commtree_time

    SNARK:
      avg_parents = (expander_degree + butterfly_degree)/2
      // TODO: note this 
      commr_inclusions = porep_challenges * (1 + avg_parents) 
      commr_inclusion_constraints = mtree_hash_constraints * mtree_depth // this is log8(replica_nodes)
      commr_inclusions_constraints = commr_inclusions * commr_inclusion_constraints

      commd_inclusions = porep_challenges/layers
      commd_inclusion_constraints = commd_hash_constraints * log2(replica_nodes)
      commd_inclusions_constraints = commd_inclusions * commd_inclusion_constraints

      // TODO: note this considers average!
      labeling_constraints = porep_challenges * avg_parents * kdf_constraints

      porep_constraints = labeling_constraints + commr_inclusions_constraints + commd_inclusions_constraints
      porep_partitions = porep_constraints / snark_partition
      porep_size_snark = porep_partitions * snark_size
      porep_time_snark = snark_constraint_time * porep_constraints

    Encoding:
      describe(encoding_hashes, "Mem access during encoding", access)
      Hashes:
        describe(hashes_node_expander, "Number of 32-bytes blocks hashed per node in expander layers", hashes)
        describe(hashes_node_butterfly, "Number of 32-bytes blocks hashed per node in butterfly layers", hashes)
        describe(hashes_window, "Number of 32-bytes blocks hashes in a window", hashes)
        describe(hashes_replica, "Number of 32-bytes blocks hashes in a replica", hashes)
        describe(hashes_node, "Number of 32-bytes blocks hashes per node in all layers", hashes)

        hashes_node_expander = expander_layers * expander_degree
        hashes_node_butterfly = butterfly_layers * butterfly_degree

        hashes_node = (hashes_node_expander + hashes_node_butterfly)

        hashes_window = hashes_node * window_nodes
        hashes_replica = hashes_window * windows

        describe(memaccess_replica_time, "Time to hash required to encode replica", s)
        describe(memaccess_window_time, "Time to hash required to encode window", s)
        hashes_window_time = (hashes_window * node_size) / rig_hashing_throughput_b_s
        hashes_replica_time = (hashes_replica * node_size) / rig_hashing_throughput_b_s

        describe(hash_kdf_expander_size, "Size of a KDF hash for the expander layers", bytes)
        describe(hash_kdf_butterfly_size, "Size of a KDF hash for the butterfly layers", bytes)
        hash_kdf_expander_size = expander_degree * node_size
        hash_kdf_butterfly_size = butterfly_degree * node_size

        hash_kdf_expander = expander_degree
        hash_kdf_butterfly = butterfly_degree

      MemAccess:
        describe(memaccess_window, "Number of 32-bytes mem access in a window", memaccess)
        describe(memaccess_replica, "Number of 32-bytes mem access in a replica", memaccess)
        describe(memaccess_node, "Number of 32-bytes blocks mem access per node in all layers", memaccess)

        memaccess_node = (hashes_node_expander * nodes_in_sequence + hashes_node_butterfly)
        memaccess_window = memaccess_node * window_nodes
        memaccess_replica = memaccess_window * windows

        describe(memaccess_replica_time, "Time to access memory required to encode replica", s)
        describe(memaccess_window_time, "Time to access memory required to encode window", s)
        memaccess_window_time = (memaccess_window * node_size) / rig_memaccess_throughput_b_s
        memaccess_replica_time = (memaccess_replica * node_size) / rig_memaccess_throughput_b_s

        porep_time_unseal = memaccess_window_time
        porep_time_replicate = memaccess_replica_time
        
  PoSt:
    post_soundness = expt(2, -post_lambda)
    post_challenges = windows * post_window_challenges
    post_partitions = post_constraints / snark_partition
    post_time_snark = snark_constraint_time * post_constraints
    post_proof_size = post_partitions * snark_size


    SNARK:
      post_constraints = commrlast_inclusions_constraints

      commrlast_depth = log2(replica_nodes)/log2(mtree_hash_blocks)
      commrlast_inclusions = post_challenges
      commrlast_inclusion_constraints = mtree_hash_constraints * commrlast_depth // this is log8(replica_nodes)
      commrlast_inclusions_constraints = commrlast_inclusions * commrlast_inclusion_constraints

Filecoin:
  network_size = network_size_eib * b_in_eib
  network_sectors = network_size / replica_size
  describe(network_post_size_in_pp, "Size of PoSt proofs in a proving period")
  describe(blocks_in_pp, "Number of blocks in a proving period")
  blocks_in_pp = tipset_size * (proving_period_hours * secs_in_hour) / block_time
  blocks_in_year = tipset_size * (secs_in_year) / block_time
  Proofs:
      describe(wpost_sectors, "Sectors proven in a WindowPoSt proof")
      wpost_sectors = snark_partition / post_constraints
      describe(wpost_storage_tib, "Amount of storage proven in a WindowPoSt proof", tib)
      wpost_storage_tib = wpost_sectors * replica_size_tib
      describe(wpost_footprint_b_per_tib_stored, "Size of proof (in bytes) for 1 TiB stored in WindowPoSt")
      wpost_footprint_b_per_tib_stored = snark_size / wpost_storage_tib 
      describe(porep_footprint_b_per_tib_stored, "Size of proof (in bytes) for 1 TiB stored in PoRep")
      porep_footprint_b_per_tib_stored = porep_size_snark / replica_size_tib 
      porep_footprint_b_per_gib_stored = porep_size_snark / replica_size_gib
      PoSt:
        pp_in_year = secs_in_year / (proving_period_hours * secs_in_hour)
        network_post_size_in_year = network_post_size_in_pp * pp_in_year
        network_post_size_in_year_gib = network_post_size_in_year / b_in_gib
        network_post_size_in_pp = network_sectors * post_proof_size 
        network_post_partitions_in_block = network_post_size_in_block / snark_size
        network_post_size_in_block = network_post_size_in_pp / blocks_in_pp
        network_post_size_in_block_kib = network_post_size_in_block / b_in_kib
      PoRep:
        network_porep_size = network_sectors * porep_size_snark
        network_porep_size_gib = network_porep_size / b_in_gib
        network_porep_size_in_block = network_porep_size / blocks_in_year

